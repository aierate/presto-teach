# 服务发现-主要流程介绍篇

[返回首页](../README.md)

---

## 简述

Presto 是怎么组建集群的呢？Presto 的 Coordinator 与 Worker 是怎么联系起来的呢？在配置的时候，我们配置了个服务发现的接口，它是怎么工作的呢？

这方面的资料特别少，基本上只有啃源码，但是怎么啃真是一点头绪都没有。于是我搜遍了全网，找了一个日本博主的文章。这篇文章给了我很多启发。

以下主要是对博主的文章进行翻译，然后丰富里面的流程内容，并且带上我自己的理解。

这里的代码主要是 Trino 为主 Presto 为辅结合起来看的，如果两边有明显的实现区别，会贴一下两边区别的代码，否则都以 Trino 为主，因为它们的服务注册发现都是基于 airlift 的 discovery 模块实现的。

以下内容主要是翻译自该博主的内容。然后我也添加了一些内容。

## 5个主要流程讲解

>注：服务注册发现(discovery node manager)的实现主要在密封类 DiscoveryNodeManager 中，这个是在Presto的主体工程中。其他的在 Presto 生态的基础框架中。

Presto 的 Coordinator 利用 discovery node manager 注册或更新 worker nodes 列表，从而实现集群化，它们之间的流程主要涉及以下四个：

- Register a node to the discovery server: worker的注册流程，每个worker node每 8 秒向discovery server注册一次
- Update the cache of the node list: service selector以10秒为间隔更新discovery server中注册的节点信息缓存
- Perform a health check: ailure detector: 健康检查流程，以 0.5 秒的间隔对worker nodes执行健康检查
- Update the node list of the coordinator: Coordinator更新Worker流程 discovery node manager更新worker node列表

对上面的四点，画了个大概的时序图

![](vx_images/109604199082.png)


接下来详细解读这4个流程

### worker的注册流程

io.airlift.discovery 为 Presto 提供了服务发现的功能，它有 Client 与 Server 两个角色。Coordinator 将通过使用 EmbeddedDiscoveryModule 实现发现服务器的功能。此外 Coordinator 和 Worker 都通过DiscoveryModule实现了client角色功能。

再理解这句话之前，需要知道几个细节，是怎么成为 Coordinator 节点或 Worker 节点的，并且怎么注册到 Discover 服务中的。

通过查看 Airlift 项目，发现 Airlift 中的 Discovery 是 Client 端，另一个在 discovery-server 项目里。是两个不同的项目。

- https://github.com/airlift/airlift 客户端的在这里面
- https://github.com/airlift/discovery 服务端的在这里面

**怎么成为Coordinator/Woker节点的呢？**

跟一下代码

```java
// io.trino.server.TrinoServer
...
new Server().start(firstNonNull(version, "unknown")); // 准备启动服务
...

// io.trino.server.Server
public final void start(String trinoVersion)
{
    new EmbedVersion(trinoVersion).embedVersion(() -> doStart(trinoVersion)).run();
}
private void doStart(String trinoVersion)
{
    ...
    ImmutableList.Builder<Module> modules = ImmutableList.builder();
    modules.add(
            new NodeModule(),
            new DiscoveryModule(), // 加载注册发现服务，注意这里是 Discovery 的 client端
            new HttpServerModule(),
            new JsonModule(),
            new JaxrsModule(),
            new MBeanModule(),
            new PrefixObjectNameGeneratorModule("io.trino"),
            new JmxModule(),
            new JmxHttpModule(),
            new LogJmxModule(),
            new TraceTokenModule(),
            new EventModule(),
            new JsonEventModule(),
            new ServerSecurityModule(),
            new AccessControlModule(),
            new EventListenerModule(),
            new CoordinatorDiscoveryModule(), // 加载 Coordinator 专属的注册发现服务，注意这里是 Discovery 的 Server 端
            new ServerMainModule(trinoVersion), // 加载节点与服务相关的主 Module 在这里确认是启动 Coordinator 还是 Worker
            new GracefulShutdownModule(),
            new WarningCollectorModule());

...

// io.trino.server.ServerMainModule
protected void setup(Binder binder)
{
    ServerConfig serverConfig = buildConfigObject(ServerConfig.class); // 根据配置中的内容
    if (serverConfig.isCoordinator()) {
        install(new CoordinatorModule()); // 成为Coordinator
    }
    else {
        install(new WorkerModule()); // 成为Worker
    }
...
```

Presto的区别

```
Trino，是在启动时，加载了大部分modules，然后有些地方再通过配置去区分。这一块基本上就像是平铺，然后再去控制。
Presto，这里算是做了一下归类，比如把CoordinatorDiscoveryModule集成到了所有与Coordinator有关的地方。
```

下面看一下代码

```java
// com.facebook.presto.server.PrestoServer

@Override
public void run()
{
    verifyJvmRequirements();
    verifySystemTimeIsReasonable();
    Logger log = Logger.get(PrestoServer.class);
    ImmutableList.Builder<Module> modules = ImmutableList.builder();
    modules.add(
            ...
            new ServerMainModule(sqlParserOptions), // 与 Coordinator Worker 角色相关的所有内容都集成在了这里面
            ...

// com.facebook.presto.server.ServerMainModule

public class ServerMainModule
        extends AbstractConfigurationAwareModule
{
    private final SqlParserOptions sqlParserOptions;

    public ServerMainModule(SqlParserOptions sqlParserOptions) { ... }

    @Override
    protected void setup(Binder binder)
    {
        ServerConfig serverConfig = buildConfigObject(ServerConfig.class);

        if (serverConfig.isResourceManager()) {
            install(new ResourceManagerModule());
        }
        else if (serverConfig.isCoordinator()) {
            install(new CoordinatorModule()); // Coordinator的注册服务也集成到里面一起了
        }
        else {
            install(new WorkerModule());
        }

// com.facebook.presto.server.CoordinatorModule

public class CoordinatorModule
        extends AbstractConfigurationAwareModule
{
    @Override
    protected void setup(Binder binder)
    {
        ...
        // discovery server
        install(installModuleIf(EmbeddedDiscoveryConfig.class, EmbeddedDiscoveryConfig::isEnabled, new EmbeddedDiscoveryModule())); // 服务注册
```

**Coordinator/Woker节点如何注册的呢？**

从上面的源码我们可以看出，是在启动服务的时候加载 Module，在 ServerMainModule 中通过配置判断启动 Coordinator 还是 Worker。

注意到，在加载 Module 的时候，我们既加载了 Discovery 的 Client，也加载了 Server。他们是怎么区分的呢？

这里有同样也有个细节，也是关于配置文件的。

在启动时，通过加载 DiscoveryModule 来注册 Client 端。在加载 CoordinatorDiscoveryModule 时里面有个对配置文件的判断。

相当于如果配置文件中指明了该节点是 Coordinator，那么会在还会加载一个 EmbeddedDiscoveryModule 来注册 Server 端。

```java
// io.trino.server.CoordinatorDiscoveryModule
protected void setup(Binder binder)
{
    if (buildConfigObject(ServerConfig.class).isCoordinator() && buildConfigObject(EmbeddedDiscoveryConfig.class).isEnabled()) {
        install(new EmbeddedDiscoveryModule());
    }
}
```

那么又有个问题了。

启动 Worker 的时候好理解，我们只注册了 Client 端。启动 Coordinator 的时候，看代码逻辑是既加载了 Client 又加载了 Server，是 Client 与 Server 都注册吗？

通过代码层面看上去是都注册，我们可以通过 `/v1/service` 接口可以来验证所有的注册信息。

现在的测试环境启动了2个节点，1个Coordinator，1个Worker。

在接口中查看到以下数据，并且发现2台节点应该是都注册的 Client，另一个节点因为是 Coordinator 它是还多注册了一个 server。

```json
{
  "environment": "cdh6_test",
  "services": [
    { ... },
    { ... },
    {
      "id": "25a99b20-bcd4-4f81-881f-8432fdd62557",
      "nodeId": "tcdh31",
      "type": "presto",
      "pool": "general",
      "location": "/tcdh31",
      "properties": {
        "node_version": "",
        "coordinator": "false",
        "http": "http://10.150.31.31:5797",
        "http-external": "http://10.150.31.31:5797",
        "connectorIds": "**"
      }
    },
    { },
    {
      "id": "924b4589-1d63-4bbe-a2f5-78a62f91a4fc",
      "nodeId": "tcdh29",
      "type": "presto-coordinator",
      "pool": "general",
      "location": "/tcdh29",
      "properties": {
        "http": "http://10.150.31.29:5797",
        "http-external": "http://10.150.31.29:5797"
      }
    },
    { ... },
    { ... },
    { ... },
    {
      "id": "d7147db7-cfba-4c57-9d7f-e10183650471",
      "nodeId": "tcdh29",
      "type": "presto",
      "pool": "general",
      "location": "/tcdh29",
      "properties": {
        "node_version": "",
        "coordinator": "true",
        "http": "http://10.150.31.29:5797",
        "http-external": "http://10.150.31.29:5797",
        "connectorIds": "***"
      }
    }
  ]
}
```

继续跟着博文走。

Client 通过向 `/v1/announcement/{node_id}` 发送 PUT 请求来向服务器注册自己。注册信息有30秒的生命周期，所以client会定期向服务器发送注册请求。这个间隔是 8 秒，源码如下：

**Airlift工程 discovery模块 客户端的发送**

```java
// io.airlift.discovery.client.Announcer 客户端发起注册的代码 发起put
private ListenableFuture<Duration> announce(long delayStart, Duration expectedDelay)
{
    // log announcement did not happen within 5 seconds of expected delay
    if (System.nanoTime() - (delayStart + expectedDelay.roundTo(NANOSECONDS)) > SECONDS.toNanos(5)) {
        log.error("Expected service announcement after %s, but announcement was delayed %s", expectedDelay, Duration.nanosSince(delayStart));
    }

    long requestStart = System.nanoTime();
    // 主要是在announce中发起请求
    ListenableFuture<Duration> future = announcementClient.announce(getServiceAnnouncements()); 

    // 通过future的回调函数异步获得结果
    Futures.addCallback(future, new FutureCallback<Duration>() 
    {
        @Override
        public void onSuccess(Duration expectedDelay)
        {
            errorBackOff.success();

            // wait 80% of the suggested delay
            expectedDelay = new Duration(expectedDelay.toMillis() * 0.8, MILLISECONDS);
            log.debug("Service announcement succeeded after %s. Next request will happen within %s", Duration.nanosSince(requestStart), expectedDelay);

            scheduleNextAnnouncement(expectedDelay);
        }

        @Override
        public void onFailure(Throwable t)
        {
            Duration duration = errorBackOff.failed(t);
            // todo this is a duplicate log message and should be remove after root cause of announcement delay is determined
            log.error("Service announcement failed after %s. Next request will happen within %s", Duration.nanosSince(requestStart), expectedDelay);
            scheduleNextAnnouncement(duration);
        }
    }, executor);

    return future;
}
```

这个每隔8秒发一次注册请求的逻辑，是一个值得学习的细节。

**discovery工程 discovery-server模块 服务端的接收**

```java
// io.airlift.discovery.server.DynamicAnnouncementResource 服务端接收注册的代码 接收put
@PUT
@Consumes(MediaType.APPLICATION_JSON)
public Response put(@PathParam("node_id") Id<Node> nodeId, @Context UriInfo uriInfo, DynamicAnnouncement announcement)
{
    if (!nodeInfo.getEnvironment().equals(announcement.getEnvironment())) {
        return Response.status(BAD_REQUEST)
                .entity(format("Environment mismatch. Expected: %s, Provided: %s", nodeInfo.getEnvironment(), announcement.getEnvironment()))
                .build();
    }

    String location = firstNonNull(announcement.getLocation(), "/somewhere/" + nodeId.toString());

    DynamicAnnouncement announcementWithLocation = DynamicAnnouncement.copyOf(announcement)
            .setLocation(location)
            .build();

    dynamicStore.put(nodeId, announcementWithLocation);

    return Response.status(ACCEPTED).build();
}
```

服务端从注册接口收到注册信息后，存入到 store 后他们是怎么控制这个30秒的生命周期呢？

```java
// io.airlift.discovery.server.ReplicatedDynamicStore 注册的内容只有30秒有效
@Override
public void put(Id<Node> nodeId, DynamicAnnouncement announcement)
{
    List<Service> services = FluentIterable.from(announcement.getServiceAnnouncements())
            .transform(toServiceWith(nodeId, announcement.getLocation(), announcement.getPool()))
            .toList();

    byte[] key = nodeId.getBytes();
    byte[] value = codec.toJsonBytes(services);

    store.put(key, value, maxAge); // 30秒设置到maxAge这个变量中了
}
```

看代码，这个 store 是一个分布式kv存储。

相当于在启动这个分布式kv存储时，会启动一个过期检查，定时检查 PUT 过来的注册信息，根据里面的生命周期条件来判断是否进行过期释放。

```java
/**
 * A simple, eventually consistent, fully replicated, distributed key-value store.
 */
public class DistributedStore {
    ...
    @Managed
    public void removeExpiredEntries()
    {
        for (Entry entry : localStore.getAll()) {
            if (isExpired(entry)) {
                localStore.delete(entry.getKey(), entry.getVersion());
            }
        }
        lastGcTimestamp.set(System.currentTimeMillis());
    }
    private boolean isExpired(Entry entry)
    {
        long ageInMs = timeSupplier.get().getMillis() - entry.getTimestamp();
        return entry.getValue() == null && ageInMs > tombstoneMaxAge.toMillis() ||  // TODO: this is repeated in StoreResource
                entry.getMaxAgeInMs() != null && ageInMs > entry.getMaxAgeInMs();
    }
    ...
}
```

看完代码，我们再搭配一下时序图，可以更好的理解流程。

![](vx_images/2122828756614.png)


最后我们还可以在日志中发现请求是每 8 秒到达一次。

```
[hadoop@ip-172-31-12-132 ~]$ grep /v1/announcement/ /var/log/presto/http-request.log | tail -6
2019-12-30T20:11:58.554Z        172.31.11.228   PUT     /v1/announcement/i-03b318a81b0e7e111    null    i-03b318a81b0e7e111     202     653     0       0       4a9527959d254d12892a7ddb075d07af0000002c3fHTTP/1.1        0       0       -1      null
2019-12-30T20:12:02.427Z        172.31.6.91     PUT     /v1/announcement/i-0a310199806b303fd    null    i-0a310199806b303fd     202     643     0       0       4a9527959d254d12892a7ddb075d07af0000002c44HTTP/1.1        0       0       -1      null
2019-12-30T20:12:03.673Z        172.31.12.132   PUT     /v1/announcement/i-0a4878ec8276e705a    null    i-0a4878ec8276e705a     202     968     0       1       4a9527959d254d12892a7ddb075d07af0000002c46HTTP/1.1        0       0       -1      null
2019-12-30T20:12:06.555Z        172.31.11.228   PUT     /v1/announcement/i-03b318a81b0e7e111    null    i-03b318a81b0e7e111     202     653     0       1       4a9527959d254d12892a7ddb075d07af0000002c49HTTP/1.1        0       0       -1      null
2019-12-30T20:12:10.429Z        172.31.6.91     PUT     /v1/announcement/i-0a310199806b303fd    null    i-0a310199806b303fd     202     643     0       0       4a9527959d254d12892a7ddb075d07af0000002c4bHTTP/1.1        0       0       -1      null
```


### worker的缓存信息更新流程

DiscoveryModule 使用 CachingServiceSelector 获取有关在发现服务器上注册的服务的信息。

主服务启动时，会加载 DiscoveryModule 模块

```java
// io.trino.server.Server
public final void start(String trinoVersion)
{
    new EmbedVersion(trinoVersion).embedVersion(() -> doStart(trinoVersion)).run();
}
private void doStart(String trinoVersion)
{
    ...
    ImmutableList.Builder<Module> modules = ImmutableList.builder();
    modules.add(
            ...
            new DiscoveryModule(),
            ...
            ...
```

加载 DiscoveryModule 模块时，会继续加载 CachingServiceSelectorFactory 与 MergingServiceSelectorFactory 来实现缓存刷新。


```java
// io.airlift.discovery.client.DiscoveryModule
@Override
public void configure(Binder binder)
{
    ...
    // bind selector factory
    binder.bind(CachingServiceSelectorFactory.class).in(Scopes.SINGLETON);
    binder.bind(ServiceSelectorFactory.class).to(MergingServiceSelectorFactory.class).in(Scopes.SINGLETON);
    ...
}
```

CachingServiceSelector 每 10 秒向发现服务器发送一个请求以更新节点列表缓存。

```java
// io.airlift.discovery.client.CachingServiceSelector
@Override
public ListenableFuture<List<ServiceDescriptor>> refresh()
{
    ServiceDescriptors oldDescriptors = this.serviceDescriptors.get();
    ListenableFuture<ServiceDescriptors> future;
    if (oldDescriptors == null) {
        future = lookupClient.getServices(type, pool);
    }
    else {
        future = lookupClient.refreshServices(oldDescriptors);
    }
    future = chainedCallback(future, new FutureCallback<ServiceDescriptors>()
    {
        @Override
        public void onSuccess(ServiceDescriptors newDescriptors)
        {
            serviceDescriptors.set(newDescriptors);
            errorBackOff.success();
            Duration delay = newDescriptors.getMaxAge();
            if (delay == null) {
                delay = DEFAULT_DELAY; // 这个变量的默认是10秒
            }
            scheduleRefresh(delay); // 刷新
        }
        @Override
        public void onFailure(Throwable t) { ... }
    }, executor);
    ...
}
```

同样我们也可以在日志里看到更新缓存的日志。由于它是 DiscoveryModule 的一个函数，它从发现服务器获取节点列表，而不考虑 Coordinator 或 Worker。

```
[hadoop@ip-172-31-12-132 ~]$ grep /v1/service/presto /var/log/presto/http-request.log | tail -6
2019-12-30T20:13:03.923Z        172.31.11.228   GET     /v1/service/presto/general      null    i-03b318a81b0e7e111     200     0       962     1       4a9527959d254d12892a7ddb075d07af0000002c8d      HTTP/1.1  0       0       0       null
2019-12-30T20:13:11.591Z        172.31.6.91     GET     /v1/service/presto/general      null    i-0a310199806b303fd     200     0       962     1       4a9527959d254d12892a7ddb075d07af0000002c93      HTTP/1.1  0       0       0       null
2019-12-30T20:13:11.807Z        172.31.12.132   GET     /v1/service/presto/general      null    i-0a4878ec8276e705a     200     0       962     0       4a9527959d254d12892a7ddb075d07af0000002c95      HTTP/1.1  0       0       0       null
2019-12-30T20:13:13.925Z        172.31.11.228   GET     /v1/service/presto/general      null    i-03b318a81b0e7e111     200     0       962     1       4a9527959d254d12892a7ddb075d07af0000002c99      HTTP/1.1  0       0       0       null
2019-12-30T20:13:21.611Z        172.31.6.91     GET     /v1/service/presto/general      null    i-0a310199806b303fd     200     0       962     1       4a9527959d254d12892a7ddb075d07af0000002c9e      HTTP/1.1  0       0       0       null
```

### 健康检查流程

HeartbeatFailureDetector 用作故障检测器。

```java
// io.trino.server.CoordinatorModule
    @Override
    protected void setup(Binder binder)
    {
        ...
        
        // failure detector
        binder.install(new FailureDetectorModule());
        jaxrsBinder(binder).bind(NodeResource.class);
        jaxrsBinder(binder).bind(WorkerResource.class);
        httpClientBinder(binder).bindHttpClient("workerInfo", ForWorkerInfo.class);
        
        ...
    }

// io.trino.failuredetector.FailureDetectorModule
public class FailureDetectorModule
        implements Module
{
    @Override
    public void configure(Binder binder)
    {
        httpClientBinder(binder)
                .bindHttpClient("failure-detector", ForFailureDetector.class)
                .withTracing();

        configBinder(binder).bindConfig(FailureDetectorConfig.class); // 获取健康检查相关配置

        binder.bind(HeartbeatFailureDetector.class).in(Scopes.SINGLETON);

        binder.bind(FailureDetector.class)
                .to(HeartbeatFailureDetector.class)
                .in(Scopes.SINGLETON);

        ExportBinder.newExporter(binder)
                .export(HeartbeatFailureDetector.class)
                .withGeneratedName();
    }
}
```

FailureDetector 主要是定义了状态的一些枚举，具体健康检查是在 HeartbeatFailureDetector 中。

HeartbeatFailureDetector 会以一定的间隔向 worker nodes 发送 HEAD 请求来进行健康检查。

```java
// io.trino.failuredetector
public class HeartbeatFailureDetector
        implements FailureDetector
{
    ...
    
    private class MonitoringTask
    {
        ...
        private void ping()
        {
            try {
                stats.recordStart();
                httpClient.executeAsync(prepareHead().setUri(uri).build(), new ResponseHandler<Object, Exception>(){...};
            }
            catch(...)
        }
    }
    ...
}
```

默认情况下，健康检查的间隔是500ms, 1分钟的失败率超过0.1就视为异常，这些都写在 FailureDetectorConfig 配置中了。

```java
public class FailureDetectorConfig
{
    private boolean enabled = true;
    private double failureRatioThreshold = 0.1; // ~6secs of failures, given default setting of heartbeatInterval = 500ms
    private Duration heartbeatInterval = new Duration(500, TimeUnit.MILLISECONDS);
    private Duration warmupInterval = new Duration(5, TimeUnit.SECONDS);
    private Duration expirationGraceInterval = new Duration(10, TimeUnit.MINUTES);
    
    ...
}
```

判断是否出现异常的逻辑

```java
private synchronized void updateState()
{
    // is this an over/under transition?
    if (stats.getRecentFailureRatio() > failureRatioThreshold) {
        successTransitionTimestamp = null;
    }
    else if (successTransitionTimestamp == null) {
        successTransitionTimestamp = System.nanoTime();
    }
}

...

public double getRecentFailureRatio()
{
    return recentFailures.getCount() / recentRequests.getCount();
}
```

Presto的区别，这里也与前面的 Worker 的注册流程一样，都集成到了 ServerMainModule 中。在根据条件启动 CoordinatorModule，然后里面会加载健康检查流程

前面的流程就不贴了，主要贴一下最后的代码。

```java
public class CoordinatorModule
        extends AbstractConfigurationAwareModule
{
    @Override
    protected void setup(Binder binder)
    {
        ...
          
        // statement resource
        ...
        
        // resource for serving static content
        ...
        
        // failure detector
        binder.install(new FailureDetectorModule());
        jaxrsBinder(binder).bind(NodeResource.class);
        jaxrsBinder(binder).bind(WorkerResource.class);
        httpClientBinder(binder).bindHttpClient("workerInfo", ForWorkerInfo.class);

...
```

### Coordinator更新Worker流程

在 DiscoveryNodeManager 类中实现了管理 Coordinator 使用的 worker 节点列表。

DiscoveryNodeManager 每 5 秒运行一次 pollWorkers 来更新节点列表和每个节点的状态。

下面去启动更新的大概代码流程：

```java
// io.trino.server.Server
private void doStart(String trinoVersion)
{
    ...
    modules.add(
            ...
            new ServerMainModule(trinoVersion), 
            ...

...

// io.trino.server.ServerMainModule
protected void setup(Binder binder)
{
    ...
    
    // node manager
    discoveryBinder(binder).bindSelector("trino");
    binder.bind(DiscoveryNodeManager.class).in(Scopes.SINGLETON); // 这里绑定DiscoveryNodeManager
    binder.bind(InternalNodeManager.class).to(DiscoveryNodeManager.class).in(Scopes.SINGLETON);
    newExporter(binder).export(DiscoveryNodeManager.class).withGeneratedName();
    httpClientBinder(binder).bindHttpClient("node-manager", ForNodeManager.class)
        .withTracing()
        .withConfigDefaults(config -> {
            config.setIdleTimeout(new Duration(30, SECONDS));
            config.setRequestTimeout(new Duration(10, SECONDS));
        });
...

// io.trino.metadata.DiscoveryNodeManager
@PostConstruct
public void startPollingNodeStates()
{
    nodeStateUpdateExecutor.scheduleWithFixedDelay(() -> {
        try {
            pollWorkers();
        }
        catch (Exception e) {
            log.error(e, "Error polling state of nodes");
        }
    }, 5, 5, TimeUnit.SECONDS);
    pollWorkers();
}
```

pollWorkers流程

```java
// io.trino.metadata.DiscoveryNodeManager

// 由一个ConcurrentHashMap来维护集群中所有node的状态
// RemoteNodeState类中主要有2个成员方法，获得节点状态getNodeState，异步刷新asyncRefresh，
private final ConcurrentHashMap<String, RemoteNodeState> nodeStates = new ConcurrentHashMap<>();

private void pollWorkers()
{
    // 构建一个节点集合，活跃宕机的都收集了
    AllNodes allNodes = getAllNodes();
    Set<InternalNode> aliveNodes = ImmutableSet.<InternalNode>builder()
            .addAll(allNodes.getActiveNodes())
            .addAll(allNodes.getShuttingDownNodes())
            .build();
            
    // 获得活跃节点的id
    ImmutableSet<String> aliveNodeIds = aliveNodes.stream()
            .map(InternalNode::getNodeIdentifier)
            .collect(toImmutableSet());
    
    // 删除不存在的节点，这里使用了Sets容器的difference，来对对两个集合做差集。
    // Remove nodes that don't exist anymore
    // Make a copy to materialize the set difference
    Set<String> deadNodes = difference(nodeStates.keySet(), aliveNodeIds).immutableCopy();
    nodeStates.keySet().removeAll(deadNodes);
    
    // 把新的节点又添加回去
    // Add new nodes
    for (InternalNode node : aliveNodes) {
        nodeStates.putIfAbsent(node.getNodeIdentifier(),
                new RemoteNodeState(httpClient, uriBuilderFrom(node.getInternalUri()).appendPath("/v1/info/state").build()));
    }
    
    // 开始刷新，更新每个节点的状态
    nodeStates.values().forEach(RemoteNodeState::asyncRefresh);
    
    // 刷新内部节点，首先会打印一些日志来显示不活跃的节点，然后使用当前新的节点状态等信息，活跃、非活跃、宕机、协调组成新的allNode信息
    refreshNodesInternal();
}
```

presto的区别

```
在最后的pollWorkers流程中，有个很细节的区别。Trino还是使用的HTTP协议通信。而Presto这边支持了一种新的协议 Thrift。 想一想，为什么呢？
看了看提交记录，这是在2020年的更新，我们看看PR中都有哪些介绍。对应的PR信息 https://github.com/prestodb/presto/pull/13894

主要内容为：

认为HTTP不太可靠，我们可以使用Thrift的方式来通信。
作者贴了个基准测试图，明显的看到稳定性有所提升。

社区里另一个大哥回复：

认为这种方式对延迟特别友好，而且认为这种方式可以提高查询和数据交换的时间。

具体的详细内容可以查看PR，在看里面的讨论时，我们可以思考一个问题，流行的RPC框架有gRPC、Thrift、rpcx、async-rpcx、tarsgo、hprose等等，为什么就选择了Thrift呢？
```

针对更新这一步，主要区别的代码在此。具体 Thrift 的实现这里暂时不贴了。

```java
private void pollWorkers()
{
    ...
    // Add new nodes
    for (InternalNode node : aliveNodes) {
        switch (protocol) {
            case HTTP:
                nodeStates.putIfAbsent(node.getNodeIdentifier(), new HttpRemoteNodeState(httpClient, uriBuilderFrom(node.getInternalUri()).appendPath("/v1/info/state").build()));
                break;
            case THRIFT:
                if (node.getThriftPort().isPresent()) {
                    nodeStates.put(node.getNodeIdentifier(),
                            new ThriftRemoteNodeState(driftClient, uriBuilderFrom(node.getInternalUri()).scheme("thrift").port(node.getThriftPort().getAsInt()).build()));
                }
                else {
                    // thrift port has not yet been populated; ignore the node for now
                }
                break;
        }
    }
    ...
}
```


pollWorkers 后续的流程，先用时序图表示了，时序图如下

![](vx_images/1034546555706.png)

节点状态通过请求/v1/info/state获取。

如果您查看每个worker的日志，也可以看到 Coordinator (172.31.12.132) 每 5 秒发出一次请求。

```
[hadoop@ip-172-31-6-91 ~]$ grep /v1/info/state /var/log/presto/http-request.log | tail -3
2019-12-30T20:51:48.469Z        172.31.12.132   GET     /v1/info/state  null    null    200     0       9       0       f2b3905f1ba842e4957ad46855b567f700000048b5      HTTP/1.1        0       0       0null
2019-12-30T20:51:53.47Z 172.31.12.132   GET     /v1/info/state  null    null    200     0       9       0       f2b3905f1ba842e4957ad46855b567f700000048c2      HTTP/1.1        0       0       0       null
2019-12-30T20:51:58.47Z 172.31.12.132   GET     /v1/info/state  null    null    200     0       9       1       f2b3905f1ba842e4957ad46855b567f700000048d0      HTTP/1.1        0       0       0       null
```


## 扩展理解：如何检查没有可用的工作节点

当先前注册的节点消失时，DiscoveryNodeManager 会将以前的活动节点丢失记录到 server.log。

```java
// io.trino.metadata.DiscoveryNodeManager

private void pollWorkers()
{
    ...
    // update indexes
    refreshNodesInternal();
}

...

private synchronized void refreshNodesInternal()
{
    ...
    

    if (allNodes != null) {
            // log node that are no longer active (but not shutting down)
            SetView<InternalNode> missingNodes = difference(allNodes.getActiveNodes(), Sets.union(activeNodesBuilder.build(), shuttingDownNodesBuilder.build()));
            for (InternalNode missingNode : missingNodes) {
                log.info("Previously active node is missing: %s (last seen at %s)", missingNode.getNodeIdentifier(), missingNode.getHost());
            }
        }
        
    ...
}
```

如果了解 Presto 中服务发现的工作原理，就会知道如果发生这种情况，会有以下几种可能性：

- discovery server 已经超过 30 秒没有收到来自 Worker 的 PUT 请求 (可以在 `/v1/service/presto/general` 中查看，是否有对应的worker信息缺失)
- Worker 节点最后一分钟健康检查失败的概率超过 10%

因此，在第一种情况下，应该检查 Coordinator 的 http-request.log，看看是否有worker节点对`/v1/announcement/{node_id}`的请求。如果没有来自该 Worker 节点的请求日志，则 Worker 节点很可能已经出现了问题。第二种情况，可以查看 Worker 节点的 `http-request.log`，看是否有`HEAD /`的请求。

如果没有请求`/v1/announcement/{node_id}`，查看 Worker 节点的 `server.log`，检查是否有致命错误。

例如，如果discovery server没有响应，则会记录以下错误日志：

```
2019-12-30T21:04:12.449Z        ERROR   Discovery-4     io.airlift.discovery.client.CachingServiceSelector      Cannot connect to discovery server for refresh (presto/general): Lookup of presto failed for http://ip-172-31-12-132.ap-northeast-1.compute.internal:8889/v1/service/presto/general
2019-12-30T21:04:12.852Z        ERROR   Discovery-0     io.airlift.discovery.client.CachingServiceSelector      Cannot connect to discovery server for refresh (collector/general): Lookup of collector failed for http://ip-172-31-12-132.ap-northeast-1.compute.internal:8889/v1/service/collector/general
2019-12-30T21:04:15.523Z        ERROR   Announcer-3     io.airlift.discovery.client.Announcer   Cannot connect to discovery server for announce: Announcement failed for http://ip-172-31-12-132.ap-northeast-1.compute.internal:8889
2019-12-30T21:04:15.524Z        ERROR   Announcer-3     io.airlift.discovery.client.Announcer   Service announcement failed after 1.00m. Next request will happen within 8000.00ms
```

如果在所有worker节点上看到此类日志，则Coordinator上可能有 Stop The World。也可以检查 Coordinator 的 jvm GC 相关的信息。

## 总结


---

这篇文章主要是讲了，Presto 是怎么组建集群并且 Coordinator 与 Worker 角色与集群的几个主要通信流程。

节点如何成客户端或服务端，客户端又怎么与服务端注册并建立连接，客户端的信息如何刷新，如何保证客户端的健康心跳检查，如何更新客户端的注册信息等等，都对其进行简单的介绍，和主要代码展示。但是这里只是单纯的讲解了几个主要流程，其实还需要搭配实际使用场景来综合讲解，才能够帮助更好的理解。

比如一个查询下来，如何分发到各个 Worker，他们之间的通信关系与这些基础服务发现关系有什么区别与联系呢？

比如某个 Worker 宕机了，整个集群的通信流程会有什么样的容错操作呢？

或者 Coordinator 宕机了，Worker与其的通信是否会有自动重连呢？

有个细节我们回顾下，Presto 是使用两个项目来完成服务发现的，https://github.com/airlift/airlift 项目中的 discovery 模块提供客户端功能，https://github.com/airlift/discovery 项目中的 discovery-server 提供服务端的功能。需要区分清楚。

参考

- https://abicky.net/2019/12/31/064732/

